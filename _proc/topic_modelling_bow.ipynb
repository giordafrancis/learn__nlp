{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: A simple explanation of Topic modelling Bag of words models\n",
    "output-file: topic_modelling_bow.html\n",
    "title: Topic Modelling BOW\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46f726-329e-41a4-9d5a-22cbfcb8654c",
   "metadata": {},
   "source": [
    "## Where Topic Modelling Fits in NLP and Machine Learning\n",
    "\n",
    "1. NLP Tasks: Topic modeling is part of unsupervised learning in NLP, often used for text mining, information retrieval, and content recommendation.\n",
    "2. Machine Learning: It utilizes unsupervised machine learning algorithms to discover hidden patterns in data without predefined labels or categories.\n",
    "\n",
    "Topic modelling is a technique in natural language processing (NLP) used to uncover the underlying topics that are present in a collection of documents. It helps in identifying patterns and organizing large sets of textual data by clustering similar words and phrases into topics. This technique is particularly useful for summarizing, categorizing, and analyzing text data.\n",
    "\n",
    "## Bag of Words (BoW) Model\n",
    "\n",
    "- Definition: The Bag of Words model is a simplifying representation used in natural language processing (NLP). **In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and word order but keeping multiplicity.**\n",
    "- Feature Extraction: Texts are converted into fixed-length vectors. Common techniques include simple word counts, term frequency (TF), and term frequency-inverse document frequency (TF-IDF). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9982601-9d4a-46bc-9fed-79baf340f5e4",
   "metadata": {},
   "source": [
    "LSA (Latent Semantic Analysis), NMF (Non-negative Matrix Factorization), and LDA (Latent Dirichlet Allocation) are considered \"bag of words\" models. Why:?    \n",
    "\n",
    "- Input Representation: All three models (LSA, NMF, LDA) take a document-term matrix as input, which is constructed using the bag of words approach.\n",
    "- Word Order Ignored: The inherent characteristic of BoW models is that they do not consider the order of words. All three models operate on this premise.\n",
    "- Focus on Frequency/Presence: They focus on the frequency or presence of words in documents, which is a key aspect of the bag of words approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4820e53-13b3-45e8-940c-6e702a3e03eb",
   "metadata": {},
   "source": [
    "## Common Algorithms for Topic Modeling\n",
    "\n",
    "1. Latent Dirichlet Allocation (LDA): One of the most popular methods, which assumes that each document is a mixture of a small number of topics and that each word in the document is attributable to one of the document's topics. Probabilistic Model: LDA is a generative probabilistic model that assumes documents are mixtures of topics, and topics are mixtures of word\n",
    "2. Non-negative Matrix Factorization (NMF): A linear algebra approach that factorizes the term-document matrix into non-negative matrices.\n",
    "3. Latent Semantic Analysis (LSA): A technique that applies singular value decomposition (SVD) to the term-document matrix to reduce its dimensions and uncover the latent structure in the data.\n",
    "\n",
    "**Summary of Why They Are BoW Model**\n",
    "\n",
    "1. Input Representation: All three models (LSA, NMF, LDA) take a document-term matrix (eg CountVectorizer, TfIDF sparse matrices) as input, which is constructed using the bag of words approach.\n",
    "2. Word Order Ignored: The inherent characteristic of BoW models is that they do not consider the order of words. All three models operate on this premise.\n",
    "3. Focus on Frequency/Presence: They focus on the frequency or presence of words in documents, which is a key aspect of the bag of words approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bdd49f-b0a5-44ce-ba1f-08dd93927134",
   "metadata": {},
   "source": [
    "### Example of a simple document term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312ae3a-96cb-4548-a2cd-e395b04c24da",
   "metadata": {},
   "source": [
    "Using a sample of small docs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed995b42-e385-4b3e-834c-4eeb1d91505e",
   "metadata": {},
   "source": [
    "Using a vector that does a simple frequency count per document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27569df6-b45a-4021-94af-1bed96c7253a",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f98969-8e79-443c-b0d2-3dcb66f9cdbc",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: machin, abc, applic, measur, perceiv, error, relat, manag, user, interfac\n",
      "\n",
      "Topic: 1 \n",
      "Words: comput, respons, time, user, engin, test, survey, opinion, ep, system\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\"\n",
    "]\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming\n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess all documents\n",
    "preprocessed_documents = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Vectorize the documents using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Train the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=0)\n",
    "lda.fit(X_tfidf)\n",
    "\n",
    "# Print the topics\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic: {idx} \\nWords: {', '.join([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da07533c-ebe5-4b5d-9df2-f21ddc95da78",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "014aa281-63be-4284-9280-619b88c4d8b5",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: lab, machin, user, engin, test, human, manag, interfac, ep, system\n",
      "\n",
      "Topic: 1 \n",
      "Words: comput, opinion, survey, measur, perceiv, relat, error, user, time, respons\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NMF\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\"\n",
    "]\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming\n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess all documents\n",
    "preprocessed_documents = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Vectorize the documents using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Train the NMF model\n",
    "nmf = NMF(n_components=2, random_state=0)\n",
    "nmf.fit(X_tfidf)\n",
    "\n",
    "# Print the topics\n",
    "for idx, topic in enumerate(nmf.components_):\n",
    "    print(f\"Topic: {idx} \\nWords: {', '.join([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d47278-490e-4fb4-8234-a7d9e75cd68a",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1c3bb7-ec55-41c7-9c0d-9820b988aa15",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "opinion human comput time respons manag interfac ep user system \n",
      "\n",
      "Topic 1:\n",
      "comput opinion survey user measur perceiv relat error time respons \n",
      "\n",
      "Word-Topic Matrix (first 10 terms):\n",
      " [[ 0.09173987  0.09173987  0.21909378  0.16600004  0.32477867  0.10543331\n",
      "   0.20794293  0.26486604  0.09173987  0.09173987]\n",
      " [-0.1077054  -0.1077054   0.05139182 -0.17741717 -0.25731592  0.27463756\n",
      "  -0.23003509 -0.2010729  -0.1077054  -0.1077054 ]]\n"
     ]
    }
   ],
   "source": [
    "# LSA\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications.\",\n",
    "    \"A survey of user opinion of computer system response time.\",\n",
    "    \"The EPS user interface management system.\",\n",
    "    \"System and human system engineering testing of EPS.\",\n",
    "    \"Relation of user perceived response time to error measurement.\"\n",
    "]\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming\n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess all documents\n",
    "preprocessed_documents = [preprocess(doc) for doc in documents]\n",
    "# Vectorize the preprocessed documents using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Apply Truncated SVD (LSA)\n",
    "lsa = TruncatedSVD(n_components=2, random_state=0)\n",
    "X_lsa = lsa.fit_transform(X_tfidf)\n",
    "\n",
    "# Print the topics\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "for idx, component in enumerate(lsa.components_):\n",
    "    print(f\"Topic {idx}:\")\n",
    "    for i in component.argsort()[-10:]:\n",
    "        print(f\"{terms[i]}\", end=' ')\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print word-topic matrix to show embeddings of terms\n",
    "print(\"Word-Topic Matrix (first 10 terms):\\n\", lsa.components_[:, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c53f95-0791-443b-84ba-fef5c2e301ea",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68b4549-d681-4cbe-beed-7dca8f215f61",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c0bc69-aaf3-4990-bd41-1f694d2d70c3",
   "metadata": {},
   "source": [
    "## Term Embeddings in Topic Analysis\n",
    "\n",
    "Term embeddings are dense vector representations of words that capture semantic meanings. They are not inherently part of the traditional algorithms like LDA, NMF, and LSA. However, embeddings can be used in conjunction with these algorithms to enhance their performance. For instance:\n",
    "\n",
    "1. LDA and NMF: Typically do not use term embeddings directly. They work with the term-document matrix (from Count Vectorizer or TF-IDF).\n",
    "2. LSA: Can be considered a precursor to modern embedding techniques. It reduces the dimensionality of the term-document matrix, uncovering latent semantic structures, but does not produce embeddings in the modern sense (like Word2Vec or GloVe).\n",
    "\n",
    "### LSA and Embeddings\n",
    "\n",
    "#### LSA (Latent Semantic Analysis):\n",
    "- *Reduction to Latent Space:* LSA uses Singular Value Decomposition (SVD) to reduce the high-dimensional term-document matrix to a lower-dimensional space. This reduced space can be seen as capturing latent semantic structures.\n",
    "\n",
    "- *Document-Topic and Word-Topic Matrices:* In LSA, the matrices obtained from SVD can be viewed as embeddings in a reduced semantic space. For instance, the X_lsa matrix represents documents in terms of latent topics, while the other matrices (U, Σ, and V^T from SVD) represent relationships between terms and topics.\n",
    "\n",
    "- *Similarity to Embeddings:* The key similarity is that **both embeddings and LSA capture semantic relationships and reduce dimensionality. However, LSA’s vectors are derived from linear algebra rather than neural network-based optimization.**\n",
    "\n",
    "#### Why NMF and LDA Aren't Typically Considered Embeddings\n",
    "\n",
    "##### NMF (Non-negative Matrix Factorization):\n",
    "- *Matrix Factorization:* NMF factorizes the term-document matrix into two lower-dimensional matrices (document-topic and topic-word) with non-negative constraints. These matrices can be interpreted similarly to embeddings, but they are not typically referred to as embeddings because they are not trained in the same way as traditional embeddings.\n",
    "- *Interpretability:* NMF’s components are directly interpretable as topics, which is different from the general-purpose semantic vectors produced by embeddings.\n",
    "\n",
    "##### LDA (Latent Dirichlet Allocation):\n",
    "\n",
    "- Probabilistic Model: LDA is a generative probabilistic model that assumes documents are mixtures of topics, and topics are mixtures of words. It outputs distributions over topics for each document and distributions over words for each topic.\n",
    "- Distributional Output: The outputs (document-topic and topic-word distributions) can be high-dimensional and sparse, contrasting with the dense, fixed-length vectors of embeddings.\n",
    "- Interpretability: Like NMF, LDA’s outputs are highly interpretable in terms of topics but don’t function as general-purpose semantic embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a56eb-6bb2-433d-b540-bbc0c8d16295",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa998703-40f5-4f63-8ffc-1787ff1c4dc6",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
