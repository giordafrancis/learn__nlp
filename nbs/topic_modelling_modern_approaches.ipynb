{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9ceeb7-baab-4ed2-a5f8-8b4919caa714",
   "metadata": {},
   "source": [
    "# Topic Modelling Modern Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da1b8b6-0fb0-40df-be71-3ea24b3d6b3a",
   "metadata": {},
   "source": [
    "## Where Topic Modelling Fits in NLP and Machine Learning\n",
    "\n",
    "1. NLP Tasks: Topic modeling is part of unsupervised learning in NLP, often used for text mining, information retrieval, and content recommendation.\n",
    "2. Machine Learning: It utilizes unsupervised machine learning algorithms to discover hidden patterns in data without predefined labels or categories.\n",
    "\n",
    "Topic modelling is a technique in natural language processing (NLP) used to uncover the underlying topics that are present in a collection of documents. It helps in identifying patterns and organizing large sets of textual data by clustering similar words and phrases into topics. This technique is particularly useful for summarizing, categorizing, and analyzing text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e9b23-9bf3-42ef-84b1-2fed8ea1fd5b",
   "metadata": {},
   "source": [
    "## Code samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bdd49f-b0a5-44ce-ba1f-08dd93927134",
   "metadata": {},
   "source": [
    "### Example of a simple document term matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0c60f-849a-4302-903e-0ba7a69b9e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 0 0 1 1]\n",
      " [0 1 0 0 1 1 1 0 1]\n",
      " [0 0 1 0 0 0 0 0 1]\n",
      " [0 1 0 1 0 1 0 0 1]]\n",
      "['brown' 'dog' 'fox' 'is' 'jumps' 'lazy' 'over' 'quick' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The quick brown fox\",\n",
    "    \"jumps over the lazy dog\",\n",
    "    \"The fox\",\n",
    "    \"The dog is lazy\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array and print\n",
    "print(X.toarray())\n",
    "\n",
    "# Feature names (vocabulary)\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c5c994-0837-4eb3-9840-c9897c8254c6",
   "metadata": {},
   "source": [
    "## Term Embedding\n",
    "\n",
    "Embeddings are dense vector representations of words or phrases that capture semantic meanings. Unlike sparse representations like one-hot encoding, embeddings map words to continuous vector spaces where semantically similar words have similar representations. These vectors are typically of lower dimensions (e.g., 100-300 dimensions) compared to the vocabulary size.\n",
    "\n",
    "\n",
    "### Importance of Embeddings\n",
    "\n",
    "1. Semantic Similarity: Words with similar meanings are closer in the embedding space.\n",
    "2. Dimensionality Reduction: Embeddings reduce the dimensionality of text data while preserving meaningful relationships.\n",
    "3. Improved Performance: Embeddings improve the performance of NLP models by providing more informative features compared to traditional methods.\n",
    "\n",
    "\n",
    "### Popular Embedding Strategies\n",
    "\n",
    "1. Word2Vec: Predicts a word given its context (Skip-gram) or predicts the context given a word (CBOW).\n",
    "2. GloVe (Global Vectors for Word Representation): Uses co-occurrence statistics to learn word embeddings.\n",
    "3. FastText: Extends Word2Vec by considering subword information, which helps with out-of-vocabulary words.\n",
    "4. BERT (Bidirectional Encoder Representations from Transformers): Uses transformer-based architecture to create context-aware embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494858f3-2740-42cc-b5fd-c2bb0bd197d0",
   "metadata": {},
   "source": [
    "## Implementation Example: Word2Vec with Gensim\n",
    "\n",
    "### Code Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bfbceb-6afe-4fd5-a27c-9a4bfef9b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00515774 -0.00667028 -0.0077791   0.00831315 -0.00198292 -0.00685696\n",
      " -0.0041556   0.00514562 -0.00286997 -0.00375075  0.0016219  -0.0027771\n",
      " -0.00158482  0.0010748  -0.00297881  0.00852176  0.00391207 -0.00996176\n",
      "  0.00626142 -0.00675622  0.00076966  0.00440552 -0.00510486 -0.00211128\n",
      "  0.00809783 -0.00424503 -0.00763848  0.00926061 -0.00215612 -0.00472081\n",
      "  0.00857329  0.00428459  0.0043261   0.00928722 -0.00845554  0.00525685\n",
      "  0.00203994  0.0041895   0.00169839  0.00446543  0.0044876   0.0061063\n",
      " -0.00320303 -0.00457706 -0.00042664  0.00253447 -0.00326412  0.00605948\n",
      "  0.00415534  0.00776685  0.00257002  0.00811905 -0.00138761  0.00808028\n",
      "  0.0037181  -0.00804967 -0.00393476 -0.0024726   0.00489447 -0.00087241\n",
      " -0.00283173  0.00783599  0.00932561 -0.0016154  -0.00516075 -0.00470313\n",
      " -0.00484746 -0.00960562  0.00137242 -0.00422615  0.00252744  0.00561612\n",
      " -0.00406709 -0.00959937  0.00154715 -0.00670207  0.0024959  -0.00378173\n",
      "  0.00708048  0.00064041  0.00356198 -0.00273993 -0.00171105  0.00765502\n",
      "  0.00140809 -0.00585215 -0.00783678  0.00123305  0.00645651  0.00555797\n",
      " -0.00897966  0.00859466  0.00404816  0.00747178  0.00974917 -0.0072917\n",
      " -0.00904259  0.0058377   0.00939395  0.00350795]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import common_texts  # Example dataset\n",
    "\n",
    "# Tokenize and preprocess the text\n",
    "sentences = [simple_preprocess(\" \".join(doc)) for doc in common_texts]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Load the model\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Get the vector for a specific word\n",
    "vector = model.wv['computer']\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8324f4-a5ca-4fa5-b645-5bbc970303ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'computer',\n",
       "  'eps',\n",
       "  'graph',\n",
       "  'human',\n",
       "  'interface',\n",
       "  'minors',\n",
       "  'response',\n",
       "  'survey',\n",
       "  'system',\n",
       "  'time',\n",
       "  'trees',\n",
       "  'user'},\n",
       " 12)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uwords = set(w for doc in common_texts for w in doc)\n",
    "uwords, len(uwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf4c149-5006-4fea-a92d-d49e05760356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyedVectors<vector_size=100, 12 keys>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.wv)\n",
    "len(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11951578-6d00-44fe-b46f-dd37019f0c45",
   "metadata": {},
   "source": [
    "## Implementation using BerTopic\n",
    "\n",
    "BerTopic docs are a great reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c6958-b492-492b-bae0-e1aad5c29266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "docs = newsgroups_train.data\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Fit the model\n",
    "topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "# Get the topics\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eabb346-0f80-4477-92b8-33b629de038b",
   "metadata": {},
   "source": [
    "We can extract info  at a document level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398d61e-ac79-4a55-9dc3-e13cc6b80ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per document info\n",
    "topic_model.get_document_info(docs).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7511912-cd4b-4c94-9558-a22c00462b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# topic_model.get_topic_info???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d1dc8-f4f4-4e2c-8fb5-a9ac44fef590",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = topic_model.get_topic_info()\n",
    "row = topic_df.sample(1)\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0559bc-0709-40a3-a5fc-93d49ddbb0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_topics, similarity = topic_model.find_topics(\"car\", top_n=5)\n",
    "print(similar_topics)\n",
    "topic_model.get_topic(similar_topics[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
