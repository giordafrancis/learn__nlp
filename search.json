[
  {
    "objectID": "bertopic_ex.html",
    "href": "bertopic_ex.html",
    "title": "Example using BerTopic",
    "section": "",
    "text": "Lets import the required libraries\n\nimport gradio as gr\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom pathlib import Path\nimport polars as pl\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.datasets import fetch_20newsgroups\nfrom umap import UMAP\nfrom bertopic import BERTopic\n\n\n# Load dataset\nnewsgroups_train = fetch_20newsgroups(subset='train')\ndocs = np.array(newsgroups_train.data)\n# subset the data as to reduce the size of docs\nidxs = np.random.randint(0, len(docs), size=200)\ndocs = docs[idxs]\n\n\ndef initial_clean(docs):\n    \"\"\"\n    A collcetion of regexs that clean text\n    \"\"\"\n    idx = 57\n    # some text cleaning example\n    html_pattern_regex = r'&lt;.*?&gt;|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});|\\xa0|&nbsp;'\n    html_start_pattern_end_dots_regex = r'&lt;(.*?)\\.\\.'\n    email_pattern_regex = r'\\S*@\\S*\\s?'\n    num_pattern_regex = r'[0-9]+'\n    nums_two_more_regex = r'\\b[0-9]{2,}\\b|\\b[0-9]+\\s[0-9]+\\b'\n    postcode_pattern_regex = r'(\\b(?:[A-Z][A-HJ-Y]?[0-9][0-9A-Z]? ?[0-9][A-Z]{2})|((GIR ?0A{2})\\b$)|(?:[A-Z][A-HJ-Y]?[0-9][0-9A-Z]? ?[0-9]{1}?)$)|(\\b(?:[A-Z][A-HJ-Y]?[0-9][0-9A-Z]?)\\b$)'\n    multiple_spaces_regex = r'\\s{2,}'\n    text = pl.Series(docs).str.strip_chars()\n    print(\"-\"*10 +\"Raw\"+\"-\"*10 )\n    print(text[idx])\n    text = text.str.replace_all(html_pattern_regex, ' ')\n    text = text.str.replace_all(html_start_pattern_end_dots_regex, ' ')\n    text = text.str.replace_all(email_pattern_regex, ' ')\n    text = text.str.replace_all(nums_two_more_regex, ' ')\n    text = text.str.replace_all(postcode_pattern_regex, ' ')\n    print(\"-\"*10 +\"After\"+\"-\"*10 )\n    print(text[idx])\n    return text\n\n\nclean_text = initial_clean(docs)\ndocs = clean_text.to_list()\n\n----------Raw----------\nFrom: franceschi@pasadena-dc.bofa.com\nSubject: Re: Gov't break-ins (Re: 60 minutes)\nOrganization: Bank America Systems Engineering, Pasadena, CA\nLines: 20\n\nOn a Los Angeles radio station last weekend, the lawyers for the\nfamily of the MURDERED rancher said that the Los Angeles Sheriff's\nDepartment had an assessment done of the rancher's property before\nthe raid.\n\nThis strongly implies that the sheriff's department wanted the property;\nany drugs (which were not found) were only an excuse.\n\nIn Viet Nam, Lt Calley was tried and convicted of murder because his\ntroops, in a war setting, deliberately killed innocent people. It is time\nthat the domestic law enforcement agencies in this country adhere to\nstandards at least as moral as the military's.\n\nGreed killed the rancher, possibly greed killed the Davidian children.\nGovernment greed.\n\nIt is time to prosecute the leaders who perform these invasions.\n\n\nFred Franceschi   (These are my own opinions!)\n----------After----------\nFrom:  Subject: Re: Gov't break-ins (Re:   minutes)\nOrganization: Bank America Systems Engineering, Pasadena, CA\nLines:  \n\nOn a Los Angeles radio station last weekend, the lawyers for the\nfamily of the MURDERED rancher said that the Los Angeles Sheriff's\nDepartment had an assessment done of the rancher's property before\nthe raid.\n\nThis strongly implies that the sheriff's department wanted the property;\nany drugs (which were not found) were only an excuse.\n\nIn Viet Nam, Lt Calley was tried and convicted of murder because his\ntroops, in a war setting, deliberately killed innocent people. It is time\nthat the domestic law enforcement agencies in this country adhere to\nstandards at least as moral as the military's.\n\nGreed killed the rancher, possibly greed killed the Davidian children.\nGovernment greed.\n\nIt is time to prosecute the leaders who perform these invasions.\n\n\nFred Franceschi   (These are my own opinions!)",
    "crumbs": [
      "Example using BerTopic"
    ]
  },
  {
    "objectID": "bertopic_ex.html#data-processing",
    "href": "bertopic_ex.html#data-processing",
    "title": "Example using BerTopic",
    "section": "",
    "text": "Lets import the required libraries\n\nimport gradio as gr\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom pathlib import Path\nimport polars as pl\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.datasets import fetch_20newsgroups\nfrom umap import UMAP\nfrom bertopic import BERTopic\n\n\n# Load dataset\nnewsgroups_train = fetch_20newsgroups(subset='train')\ndocs = np.array(newsgroups_train.data)\n# subset the data as to reduce the size of docs\nidxs = np.random.randint(0, len(docs), size=200)\ndocs = docs[idxs]\n\n\ndef initial_clean(docs):\n    \"\"\"\n    A collcetion of regexs that clean text\n    \"\"\"\n    idx = 57\n    # some text cleaning example\n    html_pattern_regex = r'&lt;.*?&gt;|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});|\\xa0|&nbsp;'\n    html_start_pattern_end_dots_regex = r'&lt;(.*?)\\.\\.'\n    email_pattern_regex = r'\\S*@\\S*\\s?'\n    num_pattern_regex = r'[0-9]+'\n    nums_two_more_regex = r'\\b[0-9]{2,}\\b|\\b[0-9]+\\s[0-9]+\\b'\n    postcode_pattern_regex = r'(\\b(?:[A-Z][A-HJ-Y]?[0-9][0-9A-Z]? ?[0-9][A-Z]{2})|((GIR ?0A{2})\\b$)|(?:[A-Z][A-HJ-Y]?[0-9][0-9A-Z]? ?[0-9]{1}?)$)|(\\b(?:[A-Z][A-HJ-Y]?[0-9][0-9A-Z]?)\\b$)'\n    multiple_spaces_regex = r'\\s{2,}'\n    text = pl.Series(docs).str.strip_chars()\n    print(\"-\"*10 +\"Raw\"+\"-\"*10 )\n    print(text[idx])\n    text = text.str.replace_all(html_pattern_regex, ' ')\n    text = text.str.replace_all(html_start_pattern_end_dots_regex, ' ')\n    text = text.str.replace_all(email_pattern_regex, ' ')\n    text = text.str.replace_all(nums_two_more_regex, ' ')\n    text = text.str.replace_all(postcode_pattern_regex, ' ')\n    print(\"-\"*10 +\"After\"+\"-\"*10 )\n    print(text[idx])\n    return text\n\n\nclean_text = initial_clean(docs)\ndocs = clean_text.to_list()\n\n----------Raw----------\nFrom: franceschi@pasadena-dc.bofa.com\nSubject: Re: Gov't break-ins (Re: 60 minutes)\nOrganization: Bank America Systems Engineering, Pasadena, CA\nLines: 20\n\nOn a Los Angeles radio station last weekend, the lawyers for the\nfamily of the MURDERED rancher said that the Los Angeles Sheriff's\nDepartment had an assessment done of the rancher's property before\nthe raid.\n\nThis strongly implies that the sheriff's department wanted the property;\nany drugs (which were not found) were only an excuse.\n\nIn Viet Nam, Lt Calley was tried and convicted of murder because his\ntroops, in a war setting, deliberately killed innocent people. It is time\nthat the domestic law enforcement agencies in this country adhere to\nstandards at least as moral as the military's.\n\nGreed killed the rancher, possibly greed killed the Davidian children.\nGovernment greed.\n\nIt is time to prosecute the leaders who perform these invasions.\n\n\nFred Franceschi   (These are my own opinions!)\n----------After----------\nFrom:  Subject: Re: Gov't break-ins (Re:   minutes)\nOrganization: Bank America Systems Engineering, Pasadena, CA\nLines:  \n\nOn a Los Angeles radio station last weekend, the lawyers for the\nfamily of the MURDERED rancher said that the Los Angeles Sheriff's\nDepartment had an assessment done of the rancher's property before\nthe raid.\n\nThis strongly implies that the sheriff's department wanted the property;\nany drugs (which were not found) were only an excuse.\n\nIn Viet Nam, Lt Calley was tried and convicted of murder because his\ntroops, in a war setting, deliberately killed innocent people. It is time\nthat the domestic law enforcement agencies in this country adhere to\nstandards at least as moral as the military's.\n\nGreed killed the rancher, possibly greed killed the Davidian children.\nGovernment greed.\n\nIt is time to prosecute the leaders who perform these invasions.\n\n\nFred Franceschi   (These are my own opinions!)",
    "crumbs": [
      "Example using BerTopic"
    ]
  },
  {
    "objectID": "bertopic_ex.html#embeddings",
    "href": "bertopic_ex.html#embeddings",
    "title": "Example using BerTopic",
    "section": "Embeddings",
    "text": "Embeddings\n\n# choosing BerTopic embeddings\nembeddings_name = \"BAAI/bge-small-en-v1.5\" \nembedding_model = SentenceTransformer(embeddings_name) \nembeddings_out  = embedding_model.encode(sentences=docs, show_progress_bar=True, batch_size=32)\n\n\n\n\n\nReduce Document embeddings matrix size\n\numap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', low_memory=False, random_state=42)\nvectoriser_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2), min_df=0.1, max_df=0.95)\nmin_docs_slider, max_topics_slider = 5, 50 # min docs required to make topic, max number of topics",
    "crumbs": [
      "Example using BerTopic"
    ]
  },
  {
    "objectID": "bertopic_ex.html#create-a-topic-model",
    "href": "bertopic_ex.html#create-a-topic-model",
    "title": "Example using BerTopic",
    "section": "Create a Topic Model",
    "text": "Create a Topic Model\n\ntopic_model = BERTopic( embedding_model=embedding_model,\n                        vectorizer_model=vectoriser_model,\n                        umap_model=umap_model,\n                        min_topic_size = min_docs_slider,\n                        nr_topics = max_topics_slider,\n                        calculate_probabilities=True,\n                        verbose = True)\nassigned_topics, probs = topic_model.fit_transform(docs, embeddings_out)\n\n2024-05-30 16:01:07,426 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n2024-05-30 16:01:11,572 - BERTopic - Dimensionality - Completed ‚úì\n2024-05-30 16:01:11,573 - BERTopic - Cluster - Start clustering the reduced embeddings\n2024-05-30 16:01:11,592 - BERTopic - Cluster - Completed ‚úì\n2024-05-30 16:01:11,593 - BERTopic - Representation - Extracting topics from clusters using representation models.\n2024-05-30 16:01:11,721 - BERTopic - Representation - Completed ‚úì\n2024-05-30 16:01:11,722 - BERTopic - Topic reduction - Reducing number of topics\n2024-05-30 16:01:11,724 - BERTopic - Topic reduction - Reduced number of topics from 10 to 10\n\n\n\n# prob of each topic\nprobs[0].shape\n\n(9,)\n\n\n\n# topic assignment\nassigned_topics[:10]\n\n[6, -1, 5, 7, 7, 0, 6, 5, 5, 5]\n\n\n\nReduce Outliers\n\n# Calculate the c-TF-IDF representation for each outlier document and find the best matching c-TF-IDF topic representation using cosine similarity.\nassigned_topics = topic_model.reduce_outliers(docs, assigned_topics, strategy=\"embeddings\")\n\n\n# Then, update the topics to the ones that considered the new data\ntopic_model.update_topics(docs, topics=assigned_topics, vectorizer_model = vectoriser_model)\n# Tidy up topic label format a bit to have commas and spaces by default\nnew_topic_labels = topic_model.generate_topic_labels(nr_words=3, separator=\", \")\ntopic_model.set_topic_labels(new_topic_labels)\n\n2024-05-30 16:01:16,304 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n\n\n\nnew_topic_labels\n\n['0, play, shots, team',\n '1, god, jesus, say',\n '2, armenian, armenians, said',\n '3, water, riding, writes article',\n '4, clipper, chip, government',\n '5, ax, ax ax, max',\n '6, dc, development, towers',\n '7, militia, gun, government',\n '8, blood, gordon banks, banks']",
    "crumbs": [
      "Example using BerTopic"
    ]
  },
  {
    "objectID": "topic_modelling_bow.html",
    "href": "topic_modelling_bow.html",
    "title": "Topic Modelling BOW",
    "section": "",
    "text": "test a link\n\nNLP Tasks: Topic modeling is part of unsupervised learning in NLP, often used for text mining, information retrieval, and content recommendation.\nMachine Learning: It utilizes unsupervised machine learning algorithms to discover hidden patterns in data without predefined labels or categories.\n\nTopic modelling is a technique in natural language processing (NLP) used to uncover the underlying topics that are present in a collection of documents. It helps in identifying patterns and organizing large sets of textual data by clustering similar words and phrases into topics. This technique is particularly useful for summarizing, categorizing, and analyzing text data.",
    "crumbs": [
      "Topic Modelling BOW"
    ]
  },
  {
    "objectID": "topic_modelling_bow.html#where-topic-modelling-fits-in-nlp-and-machine-learning",
    "href": "topic_modelling_bow.html#where-topic-modelling-fits-in-nlp-and-machine-learning",
    "title": "Topic Modelling BOW",
    "section": "",
    "text": "test a link\n\nNLP Tasks: Topic modeling is part of unsupervised learning in NLP, often used for text mining, information retrieval, and content recommendation.\nMachine Learning: It utilizes unsupervised machine learning algorithms to discover hidden patterns in data without predefined labels or categories.\n\nTopic modelling is a technique in natural language processing (NLP) used to uncover the underlying topics that are present in a collection of documents. It helps in identifying patterns and organizing large sets of textual data by clustering similar words and phrases into topics. This technique is particularly useful for summarizing, categorizing, and analyzing text data.",
    "crumbs": [
      "Topic Modelling BOW"
    ]
  },
  {
    "objectID": "topic_modelling_bow.html#bag-of-words-bow-model",
    "href": "topic_modelling_bow.html#bag-of-words-bow-model",
    "title": "Topic Modelling BOW",
    "section": "Bag of Words (BoW) Model",
    "text": "Bag of Words (BoW) Model\n\nDefinition: The Bag of Words model is a simplifying representation used in natural language processing (NLP). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and word order but keeping multiplicity.\nFeature Extraction: Texts are converted into fixed-length vectors. Common techniques include simple word counts, term frequency (TF), and term frequency-inverse document frequency (TF-IDF).\n\nLSA (Latent Semantic Analysis), NMF (Non-negative Matrix Factorization), and LDA (Latent Dirichlet Allocation) are considered ‚Äúbag of words‚Äù models. Why:?\n\nInput Representation: All three models (LSA, NMF, LDA) take a document-term matrix as input, which is constructed using the bag of words approach.\nWord Order Ignored: The inherent characteristic of BoW models is that they do not consider the order of words. All three models operate on this premise.\nFocus on Frequency/Presence: They focus on the frequency or presence of words in documents, which is a key aspect of the bag of words approach.",
    "crumbs": [
      "Topic Modelling BOW"
    ]
  },
  {
    "objectID": "topic_modelling_bow.html#common-algorithms-for-topic-modeling",
    "href": "topic_modelling_bow.html#common-algorithms-for-topic-modeling",
    "title": "Topic Modelling BOW",
    "section": "Common Algorithms for Topic Modeling",
    "text": "Common Algorithms for Topic Modeling\n\nLatent Dirichlet Allocation (LDA): One of the most popular methods, which assumes that each document is a mixture of a small number of topics and that each word in the document is attributable to one of the document‚Äôs topics. Probabilistic Model: LDA is a generative probabilistic model that assumes documents are mixtures of topics, and topics are mixtures of word\nNon-negative Matrix Factorization (NMF): A linear algebra approach that factorizes the term-document matrix into non-negative matrices.\nLatent Semantic Analysis (LSA): A technique that applies singular value decomposition (SVD) to the term-document matrix to reduce its dimensions and uncover the latent structure in the data.\n\nSummary of Why They Are BoW Model\n\nInput Representation: All three models (LSA, NMF, LDA) take a document-term matrix (eg CountVectorizer, TfIDF sparse matrices) as input, which is constructed using the bag of words approach.\nWord Order Ignored: The inherent characteristic of BoW models is that they do not consider the order of words. All three models operate on this premise.\nFocus on Frequency/Presence: They focus on the frequency or presence of words in documents, which is a key aspect of the bag of words approach.\n\n\nExample of a simple document term matrix\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nUsing a sample of small docs\n\ndocuments = [\n    \"The quick brown fox\",\n    \"jumps over the lazy dog\",\n    \"The fox\",\n    \"The dog is lazy\"\n]\n\nUsing a vector that does a simple frequency count per document\n\nvectorizer = CountVectorizer()\n\n# Fit and transform the documents\nX = vectorizer.fit_transform(documents)\n\n# Convert to array and print\nprint(X.toarray())\n\n# Feature names (vocabulary)\nprint(vectorizer.get_feature_names_out())\n\n[[1 0 1 0 0 0 0 1 1]\n [0 1 0 0 1 1 1 0 1]\n [0 0 1 0 0 0 0 0 1]\n [0 1 0 1 0 1 0 0 1]]\n['brown' 'dog' 'fox' 'is' 'jumps' 'lazy' 'over' 'quick' 'the']\n\n\n\n\nLDA\n\n# LDA\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport nltk\n\n# Sample documents\ndocuments = [\n    \"Human machine interface for lab abc computer applications\",\n    \"A survey of user opinion of computer system response time\",\n    \"The EPS user interface management system\",\n    \"System and human system engineering testing of EPS\",\n    \"Relation of user perceived response time to error measurement\"\n]\n\n# Preprocessing function\ndef preprocess(text):\n    # Lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'\\W', ' ', text)\n    # Tokenize\n    tokens = word_tokenize(text)\n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    # Stemming\n    ps = PorterStemmer()\n    tokens = [ps.stem(word) for word in tokens]\n    return ' '.join(tokens)\n\n# Preprocess all documents\npreprocessed_documents = [preprocess(doc) for doc in documents]\n\n# Vectorize the documents using TF-IDF\ntfidf_vectorizer = TfidfVectorizer()\nX_tfidf = tfidf_vectorizer.fit_transform(preprocessed_documents)\n\n# Train the LDA model\nlda = LatentDirichletAllocation(n_components=2, random_state=0)\nlda.fit(X_tfidf)\n\n# Print the topics\nfor idx, topic in enumerate(lda.components_):\n    print(f\"Topic: {idx} \\nWords: {', '.join([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])}\\n\")\n\nTopic: 0 \nWords: machin, abc, applic, measur, perceiv, error, relat, manag, user, interfac\n\nTopic: 1 \nWords: comput, respons, time, user, engin, test, survey, opinion, ep, system\n\n\n\n\n\nNMF\n\n# NMF\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport nltk\n\n# Sample documents\ndocuments = [\n    \"Human machine interface for lab abc computer applications\",\n    \"A survey of user opinion of computer system response time\",\n    \"The EPS user interface management system\",\n    \"System and human system engineering testing of EPS\",\n    \"Relation of user perceived response time to error measurement\"\n]\n\n# Preprocessing function\ndef preprocess(text):\n    # Lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'\\W', ' ', text)\n    # Tokenize\n    tokens = word_tokenize(text)\n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    # Stemming\n    ps = PorterStemmer()\n    tokens = [ps.stem(word) for word in tokens]\n    return ' '.join(tokens)\n\n# Preprocess all documents\npreprocessed_documents = [preprocess(doc) for doc in documents]\n\n# Vectorize the documents using TF-IDF\ntfidf_vectorizer = TfidfVectorizer()\nX_tfidf = tfidf_vectorizer.fit_transform(preprocessed_documents)\n\n# Train the NMF model\nnmf = NMF(n_components=2, random_state=0)\nnmf.fit(X_tfidf)\n\n# Print the topics\nfor idx, topic in enumerate(nmf.components_):\n    print(f\"Topic: {idx} \\nWords: {', '.join([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])}\\n\")\n\nTopic: 0 \nWords: lab, machin, user, engin, test, human, manag, interfac, ep, system\n\nTopic: 1 \nWords: comput, opinion, survey, measur, perceiv, relat, error, user, time, respons\n\n\n\n\n\nLSA\n\n# LSA\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport nltk\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Sample documents\ndocuments = [\n    \"Human machine interface for lab abc computer applications.\",\n    \"A survey of user opinion of computer system response time.\",\n    \"The EPS user interface management system.\",\n    \"System and human system engineering testing of EPS.\",\n    \"Relation of user perceived response time to error measurement.\"\n]\n\n# Preprocessing function\ndef preprocess(text):\n    # Lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'\\W', ' ', text)\n    # Tokenize\n    tokens = word_tokenize(text)\n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    # Stemming\n    ps = PorterStemmer()\n    tokens = [ps.stem(word) for word in tokens]\n    return ' '.join(tokens)\n\n# Preprocess all documents\npreprocessed_documents = [preprocess(doc) for doc in documents]\n# Vectorize the preprocessed documents using TF-IDF\ntfidf_vectorizer = TfidfVectorizer()\nX_tfidf = tfidf_vectorizer.fit_transform(preprocessed_documents)\n\n# Apply Truncated SVD (LSA)\nlsa = TruncatedSVD(n_components=2, random_state=0)\nX_lsa = lsa.fit_transform(X_tfidf)\n\n# Print the topics\nterms = tfidf_vectorizer.get_feature_names_out()\nfor idx, component in enumerate(lsa.components_):\n    print(f\"Topic {idx}:\")\n    for i in component.argsort()[-10:]:\n        print(f\"{terms[i]}\", end=' ')\n    print(\"\\n\")\n\n# Print word-topic matrix to show embeddings of terms\nprint(\"Word-Topic Matrix (first 10 terms):\\n\", lsa.components_[:, :10])\n\nTopic 0:\nopinion human comput time respons manag interfac ep user system \n\nTopic 1:\ncomput opinion survey user measur perceiv relat error time respons \n\nWord-Topic Matrix (first 10 terms):\n [[ 0.09173987  0.09173987  0.21909378  0.16600004  0.32477867  0.10543331\n   0.20794293  0.26486604  0.09173987  0.09173987]\n [-0.1077054  -0.1077054   0.05139182 -0.17741717 -0.25731592  0.27463756\n  -0.23003509 -0.2010729  -0.1077054  -0.1077054 ]]\n\n\n[nltk_data] Downloading package punkt to /home/frangs/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/frangs/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\nterms.shape\n\n(21,)\n\n\n\nlsa.components_.shape\n\n(2, 21)",
    "crumbs": [
      "Topic Modelling BOW"
    ]
  },
  {
    "objectID": "topic_modelling_bow.html#term-embeddings-in-topic-analysis",
    "href": "topic_modelling_bow.html#term-embeddings-in-topic-analysis",
    "title": "Topic Modelling BOW",
    "section": "Term Embeddings in Topic Analysis",
    "text": "Term Embeddings in Topic Analysis\nTerm embeddings are dense vector representations of words that capture semantic meanings. They are not inherently part of the traditional algorithms like LDA, NMF, and LSA. However, embeddings can be used in conjunction with these algorithms to enhance their performance. For instance:\n\nLDA and NMF: Typically do not use term embeddings directly. They work with the term-document matrix (from Count Vectorizer or TF-IDF).\nLSA: Can be considered a precursor to modern embedding techniques. It reduces the dimensionality of the term-document matrix, uncovering latent semantic structures, but does not produce embeddings in the modern sense (like Word2Vec or GloVe).\n\n\nLSA and Embeddings\n\nLSA (Latent Semantic Analysis):\n\nReduction to Latent Space: LSA uses Singular Value Decomposition (SVD) to reduce the high-dimensional term-document matrix to a lower-dimensional space. This reduced space can be seen as capturing latent semantic structures.\nDocument-Topic and Word-Topic Matrices: In LSA, the matrices obtained from SVD can be viewed as embeddings in a reduced semantic space. For instance, the X_lsa matrix represents documents in terms of latent topics, while the other matrices (U, Œ£, and V^T from SVD) represent relationships between terms and topics.\nSimilarity to Embeddings: The key similarity is that both embeddings and LSA capture semantic relationships and reduce dimensionality. However, LSA‚Äôs vectors are derived from linear algebra rather than neural network-based optimization.\n\n\n\nWhy NMF and LDA Aren‚Äôt Typically Considered Embeddings\n\nNMF (Non-negative Matrix Factorization):\n\nMatrix Factorization: NMF factorizes the term-document matrix into two lower-dimensional matrices (document-topic and topic-word) with non-negative constraints. These matrices can be interpreted similarly to embeddings, but they are not typically referred to as embeddings because they are not trained in the same way as traditional embeddings.\nInterpretability: NMF‚Äôs components are directly interpretable as topics, which is different from the general-purpose semantic vectors produced by embeddings.\n\n\n\nLDA (Latent Dirichlet Allocation):\n\nProbabilistic Model: LDA is a generative probabilistic model that assumes documents are mixtures of topics, and topics are mixtures of words. It outputs distributions over topics for each document and distributions over words for each topic.\nDistributional Output: The outputs (document-topic and topic-word distributions) can be high-dimensional and sparse, contrasting with the dense, fixed-length vectors of embeddings.\nInterpretability: Like NMF, LDA‚Äôs outputs are highly interpretable in terms of topics but don‚Äôt function as general-purpose semantic embeddings.",
    "crumbs": [
      "Topic Modelling BOW"
    ]
  },
  {
    "objectID": "00_vector_representations.html",
    "href": "00_vector_representations.html",
    "title": "Vector representation",
    "section": "",
    "text": "Count Vectorizer\nthis is a test for our meeting\n\n\nDefinition: Converts a collection of text documents to a matrix of token counts.\nPurpose: To represent text data as numerical data for machine learning algorithms.\n\nHow it works: - Each unique word in the corpus is assigned a unique integer index. - The output is a sparse matrix where each row represents a document and each column represents a word, with the value being the count of the word in that document.\nTF-IDF (Term Frequency-Inverse Document Frequency)\n\nDefinition: Converts a collection of raw documents to a matrix of TF-IDF features.\nPurpose: To reflect the importance of a word in a document relative to the entire corpus.\nComponents:\n\nTerm Frequency (TF): The number of times a word appears in a document, divided by the total number of words in that document.\nInverse Document Frequency (IDF): The logarithm of the total number of documents divided by the number of documents containing the word. This helps reduce the weight of common words.\n\n\nHow it works: - Words that are frequent in a document but rare in the corpus get higher scores. - The output is a sparse matrix similar to Count Vectorizer but with TF-IDF scores instead of counts.\n\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport nltk\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Sample documents\ndocuments = [\n    \"Human machine interface for lab abc computer applications.\",\n    \"A survey of user opinion of computer system response time.\",\n    \"The EPS user interface management system.\",\n    \"System and human system engineering testing of EPS.\",\n    \"Relation of user perceived response time to error measurement.\"\n]\n\n[nltk_data] Downloading package punkt to /home/frangs/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/frangs/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\n# Preprocessing function\ndef preprocess(text):\n    # Lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'\\W', ' ', text)\n    # Tokenize\n    tokens = word_tokenize(text)\n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    # Stemming\n    ps = PorterStemmer()\n    tokens = [ps.stem(word) for word in tokens]\n    return ' '.join(tokens)\n\n# Preprocess all documents\npreprocessed_documents = [preprocess(doc) for doc in documents]\n\n\npreprocessed_documents\n\n['human machin interfac lab abc comput applic',\n 'survey user opinion comput system respons time',\n 'ep user interfac manag system',\n 'system human system engin test ep',\n 'relat user perceiv respons time error measur']\n\n\n\n# Count Vectorizer Example\ncount_vectorizer = CountVectorizer()\nX_counts = count_vectorizer.fit_transform(preprocessed_documents)\n\n# TF-IDF Example\ntfidf_vectorizer = TfidfVectorizer()\nX_tfidf = tfidf_vectorizer.fit_transform(preprocessed_documents)\n\n\n# Display the vectorized data\nprint(\"Count Vectorizer Matrix:\\n\", X_counts.toarray())\nprint(\"TF-IDF Matrix:\\n\", X_tfidf.toarray())\n\n# Print the feature names (vocabulary)\nprint(\"Count Vectorizer Feature Names:\\n\", count_vectorizer.get_feature_names_out())\nprint(\"TF-IDF Feature Names:\\n\", tfidf_vectorizer.get_feature_names_out())\n\nCount Vectorizer Matrix:\n [[1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1]\n [0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1]\n [0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]\n [0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1]]\nTF-IDF Matrix:\n [[0.40986539 0.40986539 0.33067681 0.         0.         0.\n  0.33067681 0.33067681 0.40986539 0.40986539 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.        ]\n [0.         0.         0.36635462 0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.45408711 0.         0.         0.36635462 0.45408711 0.30410743\n  0.         0.36635462 0.30410743]\n [0.         0.         0.         0.         0.45109178 0.\n  0.         0.45109178 0.         0.         0.55911663 0.\n  0.         0.         0.         0.         0.         0.37444693\n  0.         0.         0.37444693]\n [0.         0.         0.         0.44298611 0.3573984  0.\n  0.3573984  0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.59334592\n  0.44298611 0.         0.        ]\n [0.         0.         0.         0.         0.         0.41701629\n  0.         0.         0.         0.         0.         0.41701629\n  0.         0.41701629 0.41701629 0.33644611 0.         0.\n  0.         0.33644611 0.27928067]]\nCount Vectorizer Feature Names:\n ['abc' 'applic' 'comput' 'engin' 'ep' 'error' 'human' 'interfac' 'lab'\n 'machin' 'manag' 'measur' 'opinion' 'perceiv' 'relat' 'respons' 'survey'\n 'system' 'test' 'time' 'user']\nTF-IDF Feature Names:\n ['abc' 'applic' 'comput' 'engin' 'ep' 'error' 'human' 'interfac' 'lab'\n 'machin' 'manag' 'measur' 'opinion' 'perceiv' 'relat' 'respons' 'survey'\n 'system' 'test' 'time' 'user']",
    "crumbs": [
      "Vector representation"
    ]
  },
  {
    "objectID": "topic_modelling_modern_approaches.html",
    "href": "topic_modelling_modern_approaches.html",
    "title": "Topic Modelling Modern Approaches",
    "section": "",
    "text": "NLP Tasks: Topic modeling is part of unsupervised learning in NLP, often used for text mining, information retrieval, and content recommendation.\nMachine Learning: It utilizes unsupervised machine learning algorithms to discover hidden patterns in data without predefined labels or categories.\n\nTopic modelling is a technique in natural language processing (NLP) used to uncover the underlying topics that are present in a collection of documents. It helps in identifying patterns and organizing large sets of textual data by clustering similar words and phrases into topics. This technique is particularly useful for summarizing, categorizing, and analyzing text data.",
    "crumbs": [
      "Topic Modelling Modern Approaches"
    ]
  },
  {
    "objectID": "topic_modelling_modern_approaches.html#where-topic-modelling-fits-in-nlp-and-machine-learning",
    "href": "topic_modelling_modern_approaches.html#where-topic-modelling-fits-in-nlp-and-machine-learning",
    "title": "Topic Modelling Modern Approaches",
    "section": "",
    "text": "NLP Tasks: Topic modeling is part of unsupervised learning in NLP, often used for text mining, information retrieval, and content recommendation.\nMachine Learning: It utilizes unsupervised machine learning algorithms to discover hidden patterns in data without predefined labels or categories.\n\nTopic modelling is a technique in natural language processing (NLP) used to uncover the underlying topics that are present in a collection of documents. It helps in identifying patterns and organizing large sets of textual data by clustering similar words and phrases into topics. This technique is particularly useful for summarizing, categorizing, and analyzing text data.",
    "crumbs": [
      "Topic Modelling Modern Approaches"
    ]
  },
  {
    "objectID": "topic_modelling_modern_approaches.html#code-samples",
    "href": "topic_modelling_modern_approaches.html#code-samples",
    "title": "Topic Modelling Modern Approaches",
    "section": "Code samples",
    "text": "Code samples\n\nExample of a simple document term matrix\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Sample documents\ndocuments = [\n    \"The quick brown fox\",\n    \"jumps over the lazy dog\",\n    \"The fox\",\n    \"The dog is lazy\"\n]\n\n# Initialize CountVectorizer\nvectorizer = CountVectorizer()\n\n# Fit and transform the documents\nX = vectorizer.fit_transform(documents)\n\n# Convert to array and print\nprint(X.toarray())\n\n# Feature names (vocabulary)\nprint(vectorizer.get_feature_names_out())\n\n[[1 0 1 0 0 0 0 1 1]\n [0 1 0 0 1 1 1 0 1]\n [0 0 1 0 0 0 0 0 1]\n [0 1 0 1 0 1 0 0 1]]\n['brown' 'dog' 'fox' 'is' 'jumps' 'lazy' 'over' 'quick' 'the']",
    "crumbs": [
      "Topic Modelling Modern Approaches"
    ]
  },
  {
    "objectID": "topic_modelling_modern_approaches.html#term-embedding",
    "href": "topic_modelling_modern_approaches.html#term-embedding",
    "title": "Topic Modelling Modern Approaches",
    "section": "Term Embedding",
    "text": "Term Embedding\nEmbeddings are dense vector representations of words or phrases that capture semantic meanings. Unlike sparse representations like one-hot encoding, embeddings map words to continuous vector spaces where semantically similar words have similar representations. These vectors are typically of lower dimensions (e.g., 100-300 dimensions) compared to the vocabulary size.\n\nImportance of Embeddings\n\nSemantic Similarity: Words with similar meanings are closer in the embedding space.\nDimensionality Reduction: Embeddings reduce the dimensionality of text data while preserving meaningful relationships.\nImproved Performance: Embeddings improve the performance of NLP models by providing more informative features compared to traditional methods.\n\n\n\nPopular Embedding Strategies\n\nWord2Vec: Predicts a word given its context (Skip-gram) or predicts the context given a word (CBOW).\nGloVe (Global Vectors for Word Representation): Uses co-occurrence statistics to learn word embeddings.\nFastText: Extends Word2Vec by considering subword information, which helps with out-of-vocabulary words.\nBERT (Bidirectional Encoder Representations from Transformers): Uses transformer-based architecture to create context-aware embeddings.",
    "crumbs": [
      "Topic Modelling Modern Approaches"
    ]
  },
  {
    "objectID": "topic_modelling_modern_approaches.html#implementation-example-word2vec-with-gensim",
    "href": "topic_modelling_modern_approaches.html#implementation-example-word2vec-with-gensim",
    "title": "Topic Modelling Modern Approaches",
    "section": "Implementation Example: Word2Vec with Gensim",
    "text": "Implementation Example: Word2Vec with Gensim\n\nCode Samples\n\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\nfrom gensim.test.utils import common_texts  # Example dataset\n\n# Tokenize and preprocess the text\nsentences = [simple_preprocess(\" \".join(doc)) for doc in common_texts]\n\n# Train Word2Vec model\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n\n# Save the model\nmodel.save(\"word2vec.model\")\n\n# Load the model\nmodel = Word2Vec.load(\"word2vec.model\")\n\n# Get the vector for a specific word\nvector = model.wv['computer']\nprint(vector)\n\n[-0.00515774 -0.00667028 -0.0077791   0.00831315 -0.00198292 -0.00685696\n -0.0041556   0.00514562 -0.00286997 -0.00375075  0.0016219  -0.0027771\n -0.00158482  0.0010748  -0.00297881  0.00852176  0.00391207 -0.00996176\n  0.00626142 -0.00675622  0.00076966  0.00440552 -0.00510486 -0.00211128\n  0.00809783 -0.00424503 -0.00763848  0.00926061 -0.00215612 -0.00472081\n  0.00857329  0.00428459  0.0043261   0.00928722 -0.00845554  0.00525685\n  0.00203994  0.0041895   0.00169839  0.00446543  0.0044876   0.0061063\n -0.00320303 -0.00457706 -0.00042664  0.00253447 -0.00326412  0.00605948\n  0.00415534  0.00776685  0.00257002  0.00811905 -0.00138761  0.00808028\n  0.0037181  -0.00804967 -0.00393476 -0.0024726   0.00489447 -0.00087241\n -0.00283173  0.00783599  0.00932561 -0.0016154  -0.00516075 -0.00470313\n -0.00484746 -0.00960562  0.00137242 -0.00422615  0.00252744  0.00561612\n -0.00406709 -0.00959937  0.00154715 -0.00670207  0.0024959  -0.00378173\n  0.00708048  0.00064041  0.00356198 -0.00273993 -0.00171105  0.00765502\n  0.00140809 -0.00585215 -0.00783678  0.00123305  0.00645651  0.00555797\n -0.00897966  0.00859466  0.00404816  0.00747178  0.00974917 -0.0072917\n -0.00904259  0.0058377   0.00939395  0.00350795]\n\n\n\nuwords = set(w for doc in common_texts for w in doc)\nuwords, len(uwords)\n\n({'computer',\n  'eps',\n  'graph',\n  'human',\n  'interface',\n  'minors',\n  'response',\n  'survey',\n  'system',\n  'time',\n  'trees',\n  'user'},\n 12)\n\n\n\nprint(model.wv)\nlen(vector)\n\nKeyedVectors&lt;vector_size=100, 12 keys&gt;\n\n\n100",
    "crumbs": [
      "Topic Modelling Modern Approaches"
    ]
  },
  {
    "objectID": "topic_modelling_modern_approaches.html#implementation-using-bertopic",
    "href": "topic_modelling_modern_approaches.html#implementation-using-bertopic",
    "title": "Topic Modelling Modern Approaches",
    "section": "Implementation using BerTopic",
    "text": "Implementation using BerTopic\nBerTopic docs are a great reference.\n\nfrom bertopic import BERTopic\nfrom sklearn.datasets import fetch_20newsgroups\nfrom pathlib import Path\nimport pickle\n\n# Load dataset\nnewsgroups_train = fetch_20newsgroups(subset='train')\ndocs = newsgroups_train.data\n\n# Initialise BERTopic\ntopic_model = BERTopic()\n\n# Fit the model\nmodel_path = Path(\"bertopic_model.pkl\")\nif not model_path.exists():\n    topics, _ = topic_model.fit_transform(docs)\n    with open(model_path, 'wb') as file:\n        pickle.dump(topic_model, file)\nelse:\n    # Save the model\n    with open(model_path, 'rb') as file:\n        topic_model  = pickle.load(file)\n# Get the topics\ntopic_model.get_topic_info().head()\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n3602\n-1_the_and_to_of\n[the, and, to, of, for, in, is, from, on, it]\n[From: tgl+@cs.cmu.edu (Tom Lane)\\nSubject: JP...\n\n\n1\n0\n486\n0_he_year_baseball_game\n[he, year, baseball, game, runs, team, pitchin...\n[From: mse@cc.bellcore.com (25836-michael even...\n\n\n2\n1\n332\n1_gun_guns_firearms_weapons\n[gun, guns, firearms, weapons, militia, amendm...\n[From: PA146008@utkvm1.utk.edu (David Veal)\\nS...\n\n\n3\n2\n310\n2_clipper_encryption_chip_key\n[clipper, encryption, chip, key, keys, escrow,...\n[Subject: text of White House announcement and...\n\n\n4\n3\n246\n3_car_cars_saturn_dealer\n[car, cars, saturn, dealer, engine, toyota, fo...\n[From: CPKJP@vm.cc.latech.edu (Kevin Parker)\\n...\n\n\n\n\n\n\n\nWe can extract info at a document level.\n\n# per document info\ntopic_model.get_document_info(docs).head()\n\n\n\n\n\n\n\n\nDocument\nTopic\nName\nRepresentation\nRepresentative_Docs\nTop_n_words\nProbability\nRepresentative_document\n\n\n\n\n0\nFrom: lerxst@wam.umd.edu (where's my thing)\\nS...\n3\n3_car_cars_saturn_dealer\n[car, cars, saturn, dealer, engine, toyota, fo...\n[From: CPKJP@vm.cc.latech.edu (Kevin Parker)\\n...\ncar - cars - saturn - dealer - engine - toyota...\n0.766315\nFalse\n\n\n1\nFrom: guykuo@carson.u.washington.edu (Guy Kuo)...\n148\n148_iisi_clock_kuo_mhz\n[iisi, clock, kuo, mhz, cpu, si, speed, guykuo...\n[From: durtralp@ux1.isu.edu (Ralph Durtschi)\\n...\niisi - clock - kuo - mhz - cpu - si - speed - ...\n0.711629\nFalse\n\n\n2\nFrom: twillis@ec.ecn.purdue.edu (Thomas E Will...\n-1\n-1_the_and_to_of\n[the, and, to, of, for, in, is, from, on, it]\n[From: tgl+@cs.cmu.edu (Tom Lane)\\nSubject: JP...\nthe - and - to - of - for - in - is - from - o...\n0.000000\nFalse\n\n\n3\nFrom: jgreen@amber (Joe Green)\\nSubject: Re: W...\n-1\n-1_the_and_to_of\n[the, and, to, of, for, in, is, from, on, it]\n[From: tgl+@cs.cmu.edu (Tom Lane)\\nSubject: JP...\nthe - and - to - of - for - in - is - from - o...\n0.000000\nFalse\n\n\n4\nFrom: jcm@head-cfa.harvard.edu (Jonathan McDow...\n-1\n-1_the_and_to_of\n[the, and, to, of, for, in, is, from, on, it]\n[From: tgl+@cs.cmu.edu (Tom Lane)\\nSubject: JP...\nthe - and - to - of - for - in - is - from - o...\n0.000000\nFalse\n\n\n\n\n\n\n\n\ntopic_df = topic_model.get_topic_info()\nrow = topic_df.sample(1)\ntopic_df.head()\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n3602\n-1_the_and_to_of\n[the, and, to, of, for, in, is, from, on, it]\n[From: tgl+@cs.cmu.edu (Tom Lane)\\nSubject: JP...\n\n\n1\n0\n486\n0_he_year_baseball_game\n[he, year, baseball, game, runs, team, pitchin...\n[From: mse@cc.bellcore.com (25836-michael even...\n\n\n2\n1\n332\n1_gun_guns_firearms_weapons\n[gun, guns, firearms, weapons, militia, amendm...\n[From: PA146008@utkvm1.utk.edu (David Veal)\\nS...\n\n\n3\n2\n310\n2_clipper_encryption_chip_key\n[clipper, encryption, chip, key, keys, escrow,...\n[Subject: text of White House announcement and...\n\n\n4\n3\n246\n3_car_cars_saturn_dealer\n[car, cars, saturn, dealer, engine, toyota, fo...\n[From: CPKJP@vm.cc.latech.edu (Kevin Parker)\\n...\n\n\n\n\n\n\n\n\nsimilar_topics, similarity = topic_model.find_topics(\"car\", top_n=5)\nprint(similar_topics)\ntopic_model.get_topic(similar_topics[0])\n\n[3, 51, 27, 185, 212]\n\n\n[('car', 0.021168209993219375),\n ('cars', 0.012855802188158174),\n ('saturn', 0.008281874562218591),\n ('dealer', 0.00795250455143509),\n ('engine', 0.00746471868549103),\n ('toyota', 0.006062030111426506),\n ('ford', 0.005955146329301384),\n ('honda', 0.005829464376667094),\n ('price', 0.005762489694584342),\n ('integra', 0.005270325883958921)]",
    "crumbs": [
      "Topic Modelling Modern Approaches"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html",
    "href": "bertopic_best_practices.html",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "",
    "text": "NOTE : this is mostly a copy from BERTopic best practices notebook where i‚Äôve placed notes that resonate with my understanding of topic modelling\nbook Through the nature of BERTopic, its modularity, many variations of the topic modeling technique is possible. However, during the development and through the usage of the package, a set of best practices have been developed that generally lead to great results.\nThe following are a number of steps, parameters, and settings that you can use that will generally improve the quality of the resulting topics. In other words, after going through the quick start and getting a feeling for the API these steps should get you to the next level of performance.\nNOTE: Although these are called best practices, it does not necessarily mean that they work across all use cases perfectly. The underlying modular nature of BERTopic is meant to take different use cases into account. After going through these practices it is advised to fine-tune wherever necessary.",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#data",
    "href": "bertopic_best_practices.html#data",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Data",
    "text": "Data\nFor this example, we will use a dataset containing abstracts and metadata from ArXiv articles.\n\nfrom datasets import load_dataset\nimport numpy as np\n\ndataset = load_dataset(\"CShorten/ML-ArXiv-Papers\")[\"train\"]\n\n# Extract abstracts to train on and corresponding titles\nabstracts = np.array(dataset[\"abstract\"])\ntitles = np.array(dataset[\"title\"])\n\nidxs = np.random.randint(0,len(titles), 20_000)\n\n\n# reducing as to allow quick embeddings creation for testing\nabstracts = abstracts[idxs]\ntitles = titles[idxs]\n\n\nabstracts[0]\n\n'  Imagine a robot is shown new concepts visually together with spoken tags,\\ne.g. \"milk\", \"eggs\", \"butter\". After seeing one paired audio-visual example per\\nclass, it is shown a new set of unseen instances of these objects, and asked to\\npick the \"milk\". Without receiving any hard labels, could it learn to match the\\nnew continuous speech input to the correct visual instance? Although unimodal\\none-shot learning has been studied, where one labelled example in a single\\nmodality is given per class, this example motivates multimodal one-shot\\nlearning. Our main contribution is to formally define this task, and to propose\\nseveral baseline and advanced models. We use a dataset of paired spoken and\\nvisual digits to specifically investigate recent advances in Siamese\\nconvolutional neural networks. Our best Siamese model achieves twice the\\naccuracy of a nearest neighbour model using pixel-distance over images and\\ndynamic time warping over speech in 11-way cross-modal matching.\\n'\n\n\nüî• Tip - Sentence Splitter üî• *** Whenever you have large documents, you typically want to split them up into either paragraphs or sentences. A nice way to do so is by using NLTK‚Äôs sentence splitter which is nothing more than:\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nsentences = [sent_tokenize(abstract) for abstract in abstracts]\nsentences = [sentence for doc in sentences for sentence in doc]\nAlso note this will make your topic analysis lose doc level context. Choose the approach based on your specific needs and the nature of your documents. For most applications, paragraph splitting provides a good balance between detail and context. ***",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#pre-calculate-embeddings",
    "href": "bertopic_best_practices.html#pre-calculate-embeddings",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Pre-calculate Embeddings",
    "text": "Pre-calculate Embeddings\nAfter having created our data, namely abstracts, we can dive into the very first best practice, pre-calculating embeddings.\nBERTopic works by converting documents into numerical values, called embeddings. This process can be very costly, especially if we want to iterate over parameters. Instead, we can calculate those embeddings once and feed them to BERTopic to skip calculating embeddings each time.\n\nfrom sentence_transformers import SentenceTransformer\n\n# Pre-calculate embeddings\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nembeddings = embedding_model.encode(abstracts, show_progress_bar=True)\n\n\n\n\n\nembeddings.shape # 384 weights per document\n\n(20000, 384)\n\n\n\nembeddings[0].shape\n\n(384,)",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#preventing-stochastic-behavior",
    "href": "bertopic_best_practices.html#preventing-stochastic-behavior",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Preventing Stochastic Behavior",
    "text": "Preventing Stochastic Behavior\nIn BERTopic, we generally use a dimensionality reduction algorithm to reduce the size of the embeddings. This is done to prevent the curse of dimensionality to a certain degree.\nAs a default, this is done with UMAP which is an incredible algorithm for reducing dimensional space. However, by default, it shows stochastic behavior which creates different results each time you run it. To prevent that, we will need to set a random_state of the model before passing it to BERTopic.\nAs a result, we can now fully reproduce the results each time we run the model.\n\nfrom umap import UMAP\n\numap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#controlling-number-of-topics",
    "href": "bertopic_best_practices.html#controlling-number-of-topics",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Controlling Number of Topics",
    "text": "Controlling Number of Topics\nThere is a parameter to control the number of topics, namely nr_topics. This parameter, however, merges topics after they have been created. It is a parameter that supports creating a fixed number of topics.\nHowever, it is advised to control the number of topics through the cluster model which is by default HDBSCAN. HDBSCAN has a parameter, namely min_topic_size that indirectly controls the number of topics that will be created.\nA higher min_topic_size will generate fewer topics and a lower min_topic_size will generate more topics.\nHere, we will go with min_topic_size=40 to get around XXX topics.\n\nfrom hdbscan import HDBSCAN\n\nhdbscan_model = HDBSCAN(min_cluster_size=150, metric='euclidean', cluster_selection_method='eom', prediction_data=True)",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#improving-default-representation",
    "href": "bertopic_best_practices.html#improving-default-representation",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Improving Default Representation",
    "text": "Improving Default Representation\nThe default representation of topics is calculated through c-TF-IDF. However, c-TF-IDF is powered by the CountVectorizer which converts text into tokens. Using the CountVectorizer, we can do a number of things:\n\nRemove stopwords\nIgnore infrequent words\nIncrease\n\nIn other words, we can preprocess the topic representations after documents are assigned to topics. This will not influence the clustering process in any way.\nHere, we will ignore English stopwords and infrequent words. Moreover, by increasing the n-gram range we will consider topic representations that are made up of one or two words.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer_model = CountVectorizer(stop_words=\"english\", \n                                   min_df=.05,\n                                   max_df=.8,\n                                   ngram_range=(1, 2))",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#additional-representations",
    "href": "bertopic_best_practices.html#additional-representations",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Additional Representations",
    "text": "Additional Representations\nPreviously, we have tuned the default representation but there are quite a number of other topic representations in BERTopic that we can choose from. From KeyBERTInspired and PartOfSpeech, to OpenAI‚Äôs ChatGPT and open-source alternatives, many representations are possible.\nIn BERTopic, you can model many different topic representations simultanously to test them out and get different perspectives of topic descriptions. This is called multi-aspect topic modeling.\nHere, we will demonstrate a number of interesting and useful representations in BERTopic:\n\nKeyBERTInspired\n\nA method that derives inspiration from how KeyBERT works\n\nPartOfSpeech\n\nUsing SpaCy‚Äôs POS tagging to extract words\n\nMaximalMarginalRelevance\n\nDiversify the topic words\n\nOpenAI\n\nUse ChatGPT to label our topics\n\n\n\n# import openai\nfrom bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n\n# KeyBERT\nkeybert_model = KeyBERTInspired()\n\n# Part-of-Speech\npos_model = PartOfSpeech(\"en_core_web_sm\")\n\n# MMR\nmmr_model = MaximalMarginalRelevance(diversity=0.3)\n\n# GPT-3.5\nprompt = \"\"\"\nI have a topic that contains the following documents:\n[DOCUMENTS]\nThe topic is described by the following keywords: [KEYWORDS]\n\nBased on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:\ntopic: &lt;topic label&gt;\n\"\"\"\n# client = openai.OpenAI(api_key=\"sk-...\")\n# openai_model = OpenAI(client, model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt)\n\n# All representation models\nrepresentation_model = {\n    \"KeyBERT\": keybert_model,\n    # \"OpenAI\": openai_model,  # Uncomment if you will use OpenAI\n    \"MMR\": mmr_model,\n    \"POS\": pos_model\n}",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#training",
    "href": "bertopic_best_practices.html#training",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Training",
    "text": "Training\nNow that we have a set of best practices, we can use them in our training loop. Here, several different representations, keywords and labels for our topics will be created. If you want to iterate over the topic model it is advised to use the pre-calculated embeddings as that significantly speeds up training.\n\nfrom bertopic import BERTopic\n\nrepresentation_model = {\n    \"KeyBERT\": keybert_model,\n    # \"OpenAI\": openai_model,  # Uncomment if you will use OpenAI\n    \"MMR\": mmr_model,\n    \"POS\": pos_model\n}\n\ntopic_model = BERTopic(\n\n  # Pipeline models\n  embedding_model=embedding_model,\n  umap_model=umap_model,\n  hdbscan_model=hdbscan_model,\n  vectorizer_model=vectorizer_model,\n  representation_model=representation_model,\n  # Hyperparameters\n  top_n_words=10,\n  verbose=True\n)\n\ntopics, probs = topic_model.fit_transform(abstracts, embeddings)\n\n2024-05-31 17:07:59,544 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n2024-05-31 17:09:13,939 - BERTopic - Dimensionality - Completed ‚úì\n2024-05-31 17:09:13,943 - BERTopic - Cluster - Start clustering the reduced embeddings\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n2024-05-31 17:09:18,259 - BERTopic - Cluster - Completed ‚úì\n2024-05-31 17:09:18,275 - BERTopic - Representation - Extracting topics from clusters using representation models.\n2024-05-31 17:10:03,879 - BERTopic - Representation - Completed ‚úì\n\n\n\ntopic_model.get_topic_info()\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nKeyBERT\nMMR\nPOS\nRepresentative_Docs\n\n\n\n\n0\n-1\n6216\n-1_quantum_tensor_eeg_communication\n[quantum, tensor, eeg, communication, forecast...\n[quantum, bayesian optimization, continual lea...\n[quantum, tensor, eeg, communication, forecast...\n[quantum, tensor, communication, forecasting, ...\n[ The quantum approximate optimization algori...\n\n\n1\n0\n2771\n0_convex_sgd_pruning_epsilon\n[convex, sgd, pruning, epsilon, norm, quantiza...\n[convex optimization, descent sgd, generalizat...\n[convex, sgd, pruning, epsilon, norm, quantiza...\n[convex, pruning, epsilon, norm, quantization,...\n[ This work characterizes the benefits of ave...\n\n\n2\n1\n2478\n1_rl_regret_reward_policies\n[rl, regret, reward, policies, robot, bandit, ...\n[deep reinforcement, policy optimization, base...\n[rl, regret, reward, policies, robot, bandit, ...\n[regret, reward, policies, robot, games, game,...\n[ We propose a generic reward shaping approac...\n\n\n3\n2\n1335\n2_bert_sentence_nlp_entity\n[bert, sentence, nlp, entity, sentiment, langu...\n[nlp tasks, entity recognition, processing nlp...\n[bert, sentence, nlp, entity, sentiment, langu...\n[sentence, entity, sentiment, documents, news,...\n[ Retrieval based open-domain QA systems use ...\n\n\n4\n3\n887\n3_attacks_anomaly_adversarial examples_anomaly...\n[attacks, anomaly, adversarial examples, anoma...\n[adversarial attack, adversarial attacks, vuln...\n[attacks, anomaly, adversarial examples, anoma...\n[attacks, anomaly, adversarial examples, pertu...\n[ Adversarial attacks for image classificatio...\n\n\n5\n4\n740\n4_privacy_federated_fl_federated learning\n[privacy, federated, fl, federated learning, c...\n[federated learning, differential privacy, pri...\n[privacy, federated, fl, federated learning, c...\n[privacy, federated, federated learning, clien...\n[ Federated learning was proposed with an int...\n\n\n6\n5\n646\n5_gnns_graph neural_gnn_node classification\n[gnns, graph neural, gnn, node classification,...\n[graph neural, graph learning, learning graphs...\n[gnns, graph neural, gnn, node classification,...\n[edges, vertices, links, walk, vertex, adjacen...\n[Node classification is a fundamental graph-ba...\n\n\n7\n6\n593\n6_covid_covid 19_ct_3d\n[covid, covid 19, ct, 3d, diagnosis, clinical,...\n[medical images, ct scans, chest ray, ct image...\n[covid, covid 19, ct, 3d, diagnosis, clinical,...\n[diagnosis, clinical, chest, cancer, tumor, di...\n[ The novel coronavirus disease 2019 (COVID-1...\n\n\n8\n7\n552\n7_audio_speaker_music_asr\n[audio, speaker, music, asr, acoustic, speech ...\n[speaker recognition, speech data, audio, audi...\n[audio, speaker, music, asr, acoustic, speech ...\n[audio, speaker, music, acoustic, voice, separ...\n[ There has been a recent surge in adversaria...\n\n\n9\n8\n413\n8_physics_differential equations_equation_fluid\n[physics, differential equations, equation, fl...\n[pdes, equations pdes, pde, artificial neural,...\n[physics, differential equations, equation, fl...\n[physics, differential equations, equation, fl...\n[ Solving analytically intractable partial di...\n\n\n10\n9\n373\n9_gp_gaussian process_gaussian processes_mcmc\n[gp, gaussian process, gaussian processes, mcm...\n[gaussian processes, sparse gaussian, deep gau...\n[gp, gaussian process, gaussian processes, mcm...\n[approximations, intractable, approximate post...\n[ Gaussian processes (GP) are Bayesian non-pa...\n\n\n11\n10\n366\n10_3d_object detection_point cloud_scene\n[3d, object detection, point cloud, scene, sem...\n[object detectors, 3d objects, 3d object, obje...\n[3d, object detection, point cloud, scene, sem...\n[object detection, scene, semantic segmentatio...\n[ This work addresses the challenging task of...\n\n\n12\n11\n319\n11_items_item_recommender_recommender systems\n[items, item, recommender, recommender systems...\n[recommendation performance, collaborative fil...\n[items, item, recommender, recommender systems...\n[items, item, recommender, preferences, rankin...\n[ The essence of the challenges cold start an...\n\n\n13\n12\n308\n12_forecasting_traffic_urban_forecast\n[forecasting, traffic, urban, forecast, travel...\n[traffic forecasting, traffic prediction, traf...\n[forecasting, traffic, urban, forecast, travel...\n[forecasting, traffic, urban, travel, transpor...\n[ Traffic problems have seriously affected pe...\n\n\n14\n13\n306\n13_explanations_counterfactual_explainability_...\n[explanations, counterfactual, explainability,...\n[counterfactual explanations, counterfactual e...\n[explanations, counterfactual, explainability,...\n[explanations, counterfactual, explainability,...\n[ By providing explanations for users and sys...\n\n\n15\n14\n302\n14_gan_discriminator_networks gans_adversarial...\n[gan, discriminator, networks gans, adversaria...\n[gan models, training gans, networks gan, gan ...\n[gan, discriminator, networks gans, adversaria...\n[discriminator, synthesis, inception, collapse...\n[ Despite the growing prominence of generativ...\n\n\n16\n15\n278\n15_patient_patients_clinical_health\n[patient, patients, clinical, health, disease,...\n[ehr data, health record, health records, reco...\n[patient, patients, clinical, health, disease,...\n[patient, patients, clinical, health, disease,...\n[ Electronic health records (EHRs) provide a ...\n\n\n17\n16\n277\n16_meta learning_shot learning_domain adaptati...\n[meta learning, shot learning, domain adaptati...\n[meta learning, meta training, meta learned, m...\n[meta learning, shot learning, domain adaptati...\n[meta learning, learner, unsupervised domain, ...\n[ In order to efficiently learn with small am...\n\n\n18\n17\n272\n17_molecular_drug_protein_molecules\n[molecular, drug, protein, molecules, chemical...\n[protein structures, drug discovery, protein s...\n[molecular, drug, protein, molecules, chemical...\n[molecular, drug, protein, molecules, chemical...\n[ The novel nature of SARS-CoV-2 calls for th...\n\n\n19\n18\n236\n18_vaes_disentangled_disentanglement_variation...\n[vaes, disentangled, disentanglement, variatio...\n[variational autoencoders, latent representati...\n[vaes, disentangled, disentanglement, variatio...\n[disentangled, disentanglement, variational au...\n[ Conditional variational autoencoders (CVAEs...\n\n\n20\n19\n178\n19_observational_confounders_observational dat...\n[observational, confounders, observational dat...\n[unobserved confounders, confounders, latent c...\n[observational, confounders, observational dat...\n[observational, confounders, observational dat...\n[ Granger causality analysis, as one of the m...\n\n\n21\n20\n154\n20_fairness_fair_discrimination_unfairness\n[fairness, fair, discrimination, unfairness, p...\n[fairness algorithms, learning fairness, fairn...\n[fairness, fair, discrimination, unfairness, p...\n[fairness, fair, discrimination, unfairness, a...\n[ Machine learning (ML) is increasingly being...\n\n\n\n\n\n\n\nTo get all representations for a single topic, we simply run the following:\n\ntopic_model.get_topic(1, full=True)\n\n{'Main': [('rl', 0.030642540433382),\n  ('regret', 0.02357864800931489),\n  ('reward', 0.023368927842700615),\n  ('policies', 0.019963649991884134),\n  ('robot', 0.01547857624288692),\n  ('bandit', 0.014575472830129322),\n  ('games', 0.012743609286372788),\n  ('game', 0.012504872215040458),\n  ('learning rl', 0.011229692897350329),\n  ('planning', 0.011050877039426714)],\n 'KeyBERT': [('deep reinforcement', 0.5913061),\n  ('policy optimization', 0.5595864),\n  ('based reinforcement', 0.5162622),\n  ('learning rl', 0.5013465),\n  ('policy gradient', 0.49714565),\n  ('imitation learning', 0.4931822),\n  ('optimal policy', 0.4656373),\n  ('reward function', 0.4315916),\n  ('reward', 0.42668962),\n  ('bandit problem', 0.41029584)],\n 'MMR': [('rl', 0.030642540433382),\n  ('regret', 0.02357864800931489),\n  ('reward', 0.023368927842700615),\n  ('policies', 0.019963649991884134),\n  ('robot', 0.01547857624288692),\n  ('bandit', 0.014575472830129322),\n  ('games', 0.012743609286372788),\n  ('game', 0.012504872215040458),\n  ('learning rl', 0.011229692897350329),\n  ('planning', 0.011050877039426714)],\n 'POS': [('regret', 0.02357864800931489),\n  ('reward', 0.023368927842700615),\n  ('policies', 0.019963649991884134),\n  ('robot', 0.01547857624288692),\n  ('games', 0.012743609286372788),\n  ('game', 0.012504872215040458),\n  ('planning', 0.011050877039426714),\n  ('arm', 0.008972612283423984),\n  ('imitation', 0.007896369253707148),\n  ('robots', 0.0074555164971588945)]}\n\n\nNOTE: The labels generated by OpenAI‚Äôs ChatGPT are especially interesting to use throughout your model. Below, we will go into more detail how to set that as a custom label.\nüî• Tip - Parameters üî•  If you would like to return the topic-document probability matrix, then it is advised to use calculate_probabilities=True. Do note that this can significantly slow down training. To speed it up, use cuML‚Äôs HDBSCAN instead. You could also approximate the topic-document probability matrix with .approximate_distribution which will be discussed later.",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#custom-labels",
    "href": "bertopic_best_practices.html#custom-labels",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "(Custom) Labels",
    "text": "(Custom) Labels\nThe default label of each topic are the top 3 words in each topic combined with an underscore between them.\nThis, of course, might not be the best label that you can think of for a certain topic. Instead, we can use .set_topic_labels to manually label all or certain topics.\nWe can also use .set_topic_labels to use one of the other topic representations that we had before, like KeyBERTInspired or even OpenAI.\n\n# Label the topics yourself\ntopic_model.set_topic_labels({1: \"Space Travel\", 7: \"Religion\"})\n\n# or use one of the other topic representations, like KeyBERTInspired\nkeybert_topic_labels = {topic: \" | \".join(list(zip(*values))[0][:3]) for topic, values in topic_model.topic_aspects_[\"KeyBERT\"].items()}\ntopic_model.set_topic_labels(keybert_topic_labels)\n\n\nkeybert_topic_labels\n\n{-1: 'quantum | bayesian optimization | continual learning',\n 0: 'convex optimization | descent sgd | generalization performance',\n 1: 'deep reinforcement | policy optimization | based reinforcement',\n 2: 'nlp tasks | entity recognition | processing nlp',\n 3: 'adversarial attack | adversarial attacks | vulnerable adversarial',\n 4: 'federated learning | differential privacy | privacy preserving',\n 5: 'graph neural | graph learning | learning graphs',\n 6: 'medical images | ct scans | chest ray',\n 7: 'speaker recognition | speech data | audio',\n 8: 'pdes | equations pdes | pde',\n 9: 'gaussian processes | sparse gaussian | deep gaussian',\n 10: 'object detectors | 3d objects | 3d object',\n 11: 'recommendation performance | collaborative filtering | recommendation systems',\n 12: 'traffic forecasting | traffic prediction | traffic data',\n 13: 'counterfactual explanations | counterfactual explanation | counterfactuals',\n 14: 'gan models | training gans | networks gan',\n 15: 'ehr data | health record | health records',\n 16: 'meta learning | meta training | meta learned',\n 17: 'protein structures | drug discovery | protein structure',\n 18: 'variational autoencoders | latent representations | learned representations',\n 19: 'unobserved confounders | confounders | latent confounders',\n 20: 'fairness algorithms | learning fairness | fairness metrics'}\n\n\nNow that we have set the updated topic labels, we can access them with the many functions used throughout BERTopic. Most notably, you can show the updated labels in visualizations with the custom_labels=True parameters.\n\ntopic_model.get_topic_info()\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nCustomName\nRepresentation\nKeyBERT\nMMR\nPOS\nRepresentative_Docs\n\n\n\n\n0\n-1\n6216\n-1_quantum_tensor_eeg_communication\nquantum | bayesian optimization | continual le...\n[quantum, tensor, eeg, communication, forecast...\n[quantum, bayesian optimization, continual lea...\n[quantum, tensor, eeg, communication, forecast...\n[quantum, tensor, communication, forecasting, ...\n[ The quantum approximate optimization algori...\n\n\n1\n0\n2771\n0_convex_sgd_pruning_epsilon\nconvex optimization | descent sgd | generaliza...\n[convex, sgd, pruning, epsilon, norm, quantiza...\n[convex optimization, descent sgd, generalizat...\n[convex, sgd, pruning, epsilon, norm, quantiza...\n[convex, pruning, epsilon, norm, quantization,...\n[ This work characterizes the benefits of ave...\n\n\n2\n1\n2478\n1_rl_regret_reward_policies\ndeep reinforcement | policy optimization | bas...\n[rl, regret, reward, policies, robot, bandit, ...\n[deep reinforcement, policy optimization, base...\n[rl, regret, reward, policies, robot, bandit, ...\n[regret, reward, policies, robot, games, game,...\n[ We propose a generic reward shaping approac...\n\n\n3\n2\n1335\n2_bert_sentence_nlp_entity\nnlp tasks | entity recognition | processing nlp\n[bert, sentence, nlp, entity, sentiment, langu...\n[nlp tasks, entity recognition, processing nlp...\n[bert, sentence, nlp, entity, sentiment, langu...\n[sentence, entity, sentiment, documents, news,...\n[ Retrieval based open-domain QA systems use ...\n\n\n4\n3\n887\n3_attacks_anomaly_adversarial examples_anomaly...\nadversarial attack | adversarial attacks | vul...\n[attacks, anomaly, adversarial examples, anoma...\n[adversarial attack, adversarial attacks, vuln...\n[attacks, anomaly, adversarial examples, anoma...\n[attacks, anomaly, adversarial examples, pertu...\n[ Adversarial attacks for image classificatio...\n\n\n5\n4\n740\n4_privacy_federated_fl_federated learning\nfederated learning | differential privacy | pr...\n[privacy, federated, fl, federated learning, c...\n[federated learning, differential privacy, pri...\n[privacy, federated, fl, federated learning, c...\n[privacy, federated, federated learning, clien...\n[ Federated learning was proposed with an int...\n\n\n6\n5\n646\n5_gnns_graph neural_gnn_node classification\ngraph neural | graph learning | learning graphs\n[gnns, graph neural, gnn, node classification,...\n[graph neural, graph learning, learning graphs...\n[gnns, graph neural, gnn, node classification,...\n[edges, vertices, links, walk, vertex, adjacen...\n[Node classification is a fundamental graph-ba...\n\n\n7\n6\n593\n6_covid_covid 19_ct_3d\nmedical images | ct scans | chest ray\n[covid, covid 19, ct, 3d, diagnosis, clinical,...\n[medical images, ct scans, chest ray, ct image...\n[covid, covid 19, ct, 3d, diagnosis, clinical,...\n[diagnosis, clinical, chest, cancer, tumor, di...\n[ The novel coronavirus disease 2019 (COVID-1...\n\n\n8\n7\n552\n7_audio_speaker_music_asr\nspeaker recognition | speech data | audio\n[audio, speaker, music, asr, acoustic, speech ...\n[speaker recognition, speech data, audio, audi...\n[audio, speaker, music, asr, acoustic, speech ...\n[audio, speaker, music, acoustic, voice, separ...\n[ There has been a recent surge in adversaria...\n\n\n9\n8\n413\n8_physics_differential equations_equation_fluid\npdes | equations pdes | pde\n[physics, differential equations, equation, fl...\n[pdes, equations pdes, pde, artificial neural,...\n[physics, differential equations, equation, fl...\n[physics, differential equations, equation, fl...\n[ Solving analytically intractable partial di...\n\n\n10\n9\n373\n9_gp_gaussian process_gaussian processes_mcmc\ngaussian processes | sparse gaussian | deep ga...\n[gp, gaussian process, gaussian processes, mcm...\n[gaussian processes, sparse gaussian, deep gau...\n[gp, gaussian process, gaussian processes, mcm...\n[approximations, intractable, approximate post...\n[ Gaussian processes (GP) are Bayesian non-pa...\n\n\n11\n10\n366\n10_3d_object detection_point cloud_scene\nobject detectors | 3d objects | 3d object\n[3d, object detection, point cloud, scene, sem...\n[object detectors, 3d objects, 3d object, obje...\n[3d, object detection, point cloud, scene, sem...\n[object detection, scene, semantic segmentatio...\n[ This work addresses the challenging task of...\n\n\n12\n11\n319\n11_items_item_recommender_recommender systems\nrecommendation performance | collaborative fil...\n[items, item, recommender, recommender systems...\n[recommendation performance, collaborative fil...\n[items, item, recommender, recommender systems...\n[items, item, recommender, preferences, rankin...\n[ The essence of the challenges cold start an...\n\n\n13\n12\n308\n12_forecasting_traffic_urban_forecast\ntraffic forecasting | traffic prediction | tra...\n[forecasting, traffic, urban, forecast, travel...\n[traffic forecasting, traffic prediction, traf...\n[forecasting, traffic, urban, forecast, travel...\n[forecasting, traffic, urban, travel, transpor...\n[ Traffic problems have seriously affected pe...\n\n\n14\n13\n306\n13_explanations_counterfactual_explainability_...\ncounterfactual explanations | counterfactual e...\n[explanations, counterfactual, explainability,...\n[counterfactual explanations, counterfactual e...\n[explanations, counterfactual, explainability,...\n[explanations, counterfactual, explainability,...\n[ By providing explanations for users and sys...\n\n\n15\n14\n302\n14_gan_discriminator_networks gans_adversarial...\ngan models | training gans | networks gan\n[gan, discriminator, networks gans, adversaria...\n[gan models, training gans, networks gan, gan ...\n[gan, discriminator, networks gans, adversaria...\n[discriminator, synthesis, inception, collapse...\n[ Despite the growing prominence of generativ...\n\n\n16\n15\n278\n15_patient_patients_clinical_health\nehr data | health record | health records\n[patient, patients, clinical, health, disease,...\n[ehr data, health record, health records, reco...\n[patient, patients, clinical, health, disease,...\n[patient, patients, clinical, health, disease,...\n[ Electronic health records (EHRs) provide a ...\n\n\n17\n16\n277\n16_meta learning_shot learning_domain adaptati...\nmeta learning | meta training | meta learned\n[meta learning, shot learning, domain adaptati...\n[meta learning, meta training, meta learned, m...\n[meta learning, shot learning, domain adaptati...\n[meta learning, learner, unsupervised domain, ...\n[ In order to efficiently learn with small am...\n\n\n18\n17\n272\n17_molecular_drug_protein_molecules\nprotein structures | drug discovery | protein ...\n[molecular, drug, protein, molecules, chemical...\n[protein structures, drug discovery, protein s...\n[molecular, drug, protein, molecules, chemical...\n[molecular, drug, protein, molecules, chemical...\n[ The novel nature of SARS-CoV-2 calls for th...\n\n\n19\n18\n236\n18_vaes_disentangled_disentanglement_variation...\nvariational autoencoders | latent representati...\n[vaes, disentangled, disentanglement, variatio...\n[variational autoencoders, latent representati...\n[vaes, disentangled, disentanglement, variatio...\n[disentangled, disentanglement, variational au...\n[ Conditional variational autoencoders (CVAEs...\n\n\n20\n19\n178\n19_observational_confounders_observational dat...\nunobserved confounders | confounders | latent ...\n[observational, confounders, observational dat...\n[unobserved confounders, confounders, latent c...\n[observational, confounders, observational dat...\n[observational, confounders, observational dat...\n[ Granger causality analysis, as one of the m...\n\n\n21\n20\n154\n20_fairness_fair_discrimination_unfairness\nfairness algorithms | learning fairness | fair...\n[fairness, fair, discrimination, unfairness, p...\n[fairness algorithms, learning fairness, fairn...\n[fairness, fair, discrimination, unfairness, p...\n[fairness, fair, discrimination, unfairness, a...\n[ Machine learning (ML) is increasingly being...\n\n\n\n\n\n\n\nNotice that the overview in .get_topic_info now also includes the column CustomName. That is the custom label that we just created for each topic.",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#topic-document-distribution",
    "href": "bertopic_best_practices.html#topic-document-distribution",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Topic-Document Distribution",
    "text": "Topic-Document Distribution\nIf using calculate_probabilities=True is not possible, than you can approximate the topic-document distributions using .approximate_distribution. It is a fast and flexible method for creating different topic-document distributions.\n\n# `topic_distr` contains the distribution of topics in each document\ntopic_distr, _ = topic_model.approximate_distribution(abstracts, window=8, stride=4)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:30&lt;00:00,  1.54s/it]\n\n\nNext, lets take a look at a specific abstract and see how the topic distribution was extracted:\n\nabstract_id = 12\nprint(abstracts[abstract_id])\n\n  We present a generic framework for spatio-temporal (ST) data modeling,\nanalysis, and forecasting, with a special focus on data that is sparse in both\nspace and time. Our multi-scaled framework is a seamless coupling of two major\ncomponents: a self-exciting point process that models the macroscale\nstatistical behaviors of the ST data and a graph structured recurrent neural\nnetwork (GSRNN) to discover the microscale patterns of the ST data on the\ninferred graph. This novel deep neural network (DNN) incorporates the real time\ninteractions of the graph nodes to enable more accurate real time forecasting.\nThe effectiveness of our method is demonstrated on both crime and traffic\nforecasting.\n\n\n\n\n# Visualize the topic-document distribution for a single document\ntopic_model.visualize_distribution(topic_distr[abstract_id])\n\n                                                \n\n\n\n# Visualize the topic-document distribution for a single document\ntopic_model.visualize_distribution(topic_distr[abstract_id], custom_labels=True)\n\n                                                \n\n\nIt seems to have extracted a number of topics that are relevant and shows the distributions of these topics across the abstract. We can go one step further and visualize them on a token-level:\n\n# Calculate the topic distributions on a token-level\ntopic_distr, topic_token_distr = topic_model.approximate_distribution(abstracts[abstract_id], calculate_tokens=True)\n\n# Visualize the token-level distributions\ndf = topic_model.visualize_approximate_distribution(abstracts[abstract_id], topic_token_distr[0])\ndf\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 31.97it/s]\n\n\n\n\n\n\n\n¬†\nWe\npresent\ngeneric\nframework\nfor\nspatio\ntemporal\nST\ndata\nmodeling\nanalysis\nand\nforecasting\nwith\nspecial\nfocus\non\ndata\nthat\nis\nsparse\nin\nboth\nspace\nand\ntime\nOur\nmulti\nscaled\nframework\nis\nseamless\ncoupling\nof\ntwo\nmajor\ncomponents\nself\nexciting\npoint\nprocess\nthat\nmodels\nthe\nmacroscale\nstatistical\nbehaviors\nof\nthe\nST\ndata\nand\ngraph\nstructured\nrecurrent\nneural\nnetwork\nGSRNN\nto\ndiscover\nthe\nmicroscale\npatterns\nof\nthe\nST\ndata\non\nthe\ninferred\ngraph\nThis\nnovel\ndeep\nneural\nnetwork\nDNN\nincorporates\nthe\nreal\ntime\ninteractions\nof\nthe\ngraph\nnodes\nto\nenable\nmore\naccurate\nreal\ntime\nforecasting\nThe\neffectiveness\nof\nour\nmethod\nis\ndemonstrated\non\nboth\ncrime\nand\ntraffic\nforecasting\n\n\n\n\n12_forecasting_traffic_urban_forecast\n0.000\n0.000\n0.000\n0.100\n0.100\n0.100\n0.100\n0.000\n0.000\n0.189\n0.604\n1.019\n1.205\n1.016\n0.601\n0.187\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.141\n0.318\n0.494\n0.909\n0.768\n0.591\n0.415\n0.000\n0.000\n0.000\n0.000\n0.000\n0.183\n0.489\n0.489\n0.489\n0.306\n\n\n\n\n\nüî• Tip - use_embedding_model üî• *** As a default, we compare the c-TF-IDF calculations between the token sets and all topics. Due to its bag-of-word representation, this is quite fast. However, you might want to use the selected embedding_model instead to do this comparison. Do note that due to the many token sets, it is often computationally quite a bit slower:\ntopic_distr, _ = topic_model.approximate_distribution(docs, use_embedding_model=True)",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#outlier-reduction",
    "href": "bertopic_best_practices.html#outlier-reduction",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Outlier Reduction",
    "text": "Outlier Reduction\nBy default, HDBSCAN generates outliers which is a helpful mechanic in creating accurate topic representations. However, you might want to assign every single document to a topic. We can use .reduce_outliers to map some or all outliers to a topic:\n\n# Reduce outliers\nnew_topics = topic_model.reduce_outliers(abstracts, topics)\n\n# Reduce outliers with pre-calculate embeddings instead\nnew_topics = topic_model.reduce_outliers(abstracts, topics, strategy=\"embeddings\", embeddings=embeddings)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25&lt;00:00,  3.63s/it]\n\n\nüí° NOTE - Update Topics with Outlier Reduction üí° *** After having generated updated topic assignments, we can pass them to BERTopic in order to update the topic representations:\ntopic_model.update_topics(docs, topics=new_topics)\nIt is important to realize that updating the topics this way may lead to errors if topic reduction or topic merging techniques are used afterwards. The reason for this is that when you assign a -1 document to topic 1 and another -1 document to topic 2, it is unclear how you map the -1 documents. Is it matched to topic 1 or 2. ***",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#visualize-topics",
    "href": "bertopic_best_practices.html#visualize-topics",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Visualize Topics",
    "text": "Visualize Topics\nWith visualizations, we are closing into the realm of subjective ‚Äúbest practices‚Äù. These are things that I generally do because I like the representations but your experience might differ.\nHaving said that, there are two visualizations that are my go-to when visualizing the topics themselves:\n\ntopic_model.visualize_topics()\ntopic_model.visualize_hierarchy()\n\n\ntopic_model.visualize_topics(custom_labels=True)\n\n                                                \n\n\n\ntopic_model.visualize_hierarchy(custom_labels=True)",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#visualize-documents",
    "href": "bertopic_best_practices.html#visualize-documents",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Visualize Documents",
    "text": "Visualize Documents\nWhen visualizing documents, it helps to have embedded the documents beforehand to speed up computation. Fortunately, we have already done that as a ‚Äúbest practice‚Äù.\nVisualizing documents in 2-dimensional space helps in understanding the underlying structure of the documents and topics.\n\n# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\nreduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n\nThe following plot is interactive which means that you can zoom in, double click on a label to only see that one and generally interact with the plot:\n\n# Visualize the documents in 2-dimensional space and show the titles on hover instead of the abstracts\n# NOTE: You can hide the hover with `hide_document_hover=True` which is especially helpful if you have a large dataset\ntopic_model.visualize_documents(titles, reduced_embeddings=reduced_embeddings, custom_labels=True)\n\n                                                \n\n\n\n# We can also hide the annotation to have a more clear overview of the topics\ntopic_model.visualize_documents(titles, reduced_embeddings=reduced_embeddings, custom_labels=True, hide_annotations=True)\n\n                                                \n\n\nüí° NOTE - 2-dimensional space üí° *** Although visualizing the documents in 2-dimensional gives an idea of their underlying structure, there is a risk involved.\nVisualizing the documents in 2-dimensional space means that we have lost significant information since the original embeddings were more than 384 dimensions. Condensing all that information in 2 dimensions is simply not possible. In other words, it is merely an approximation, albeit quite an accurate one. ***",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#serialization",
    "href": "bertopic_best_practices.html#serialization",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Serialization",
    "text": "Serialization\nWhen saving a BERTopic model, there are several ways in doing so. You can either save the entire model with pickle, pytorch, or safetensors.\nPersonally, I would advise going with safetensors whenever possible. The reason for this is that the format allows for a very small topic model to be saved and shared.\nWhen saving a model with safetensors, it skips over saving the dimensionality reduction and clustering models. The .transform function will still work without these models but instead assign topics based on the similarity between document embeddings and the topic embeddings.\nAs a result, the .transform step might give different results but it is generally worth it considering the smaller and significantly faster model.\n\nembedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\ntopic_model.save(\"my_model_dir\", serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)\n\nüí° NOTE - Embedding Model üí° *** Using safetensors, we are not saving the underlying embedding model but merely a pointer to the model. For example, in the above example we are saving the string \"sentence-transformers/all-MiniLM-L6-v2\" so that we can load in the embedding model alongside the topic model.\nThis currently only works if you are using a sentence transformer model. If you are using a different model, you can load it in when loading the topic model like this:\nfrom sentence_transformers import SentenceTransformer\n\n# Define embedding model\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Load model and add embedding model\nloaded_model = BERTopic.load(\"path/to/my/model_dir\", embedding_model=embedding_model)\n\nAs mentioned above, loading can be done as follows:\n\nfrom sentence_transformers import SentenceTransformer\n\n# Define embedding model\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Load model and add embedding model\nloaded_model = BERTopic.load(\"my_model_dir\", embedding_model=embedding_model)",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  },
  {
    "objectID": "bertopic_best_practices.html#inference",
    "href": "bertopic_best_practices.html#inference",
    "title": "Tutorial - BERTopic Best Practices",
    "section": "Inference",
    "text": "Inference\nTo speed up the inference, we can leverage a ‚Äúbest practice‚Äù that we used before, namely serialization. When you save a model as safetensors and then load it in, we are removing the dimensionality reduction and clustering steps from the pipeline.\nInstead, the assignment of topics is done through cosine similarity of document embeddings and topic embeddings. This speeds up inferences significantly.\nTo show its effect, let‚Äôs start by disabling the logger:\n\nfrom bertopic._utils import MyLogger\nlogger = MyLogger(\"ERROR\")\nloaded_model.verbose = False\ntopic_model.verbose = False\n\nThen, we run inference on both the loaded model and the non-loaded model:\n\n\n\n17.1 s ¬± 6.05 s per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\n\n\n\n12.2 s ¬± 2.07 s per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\nBased on the above, the loaded_model seems to be quite a bit faster for inference than the original topic_model.",
    "crumbs": [
      "**Tutorial** - BERTopic Best Practices"
    ]
  }
]